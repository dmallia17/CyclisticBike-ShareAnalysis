---
title: "Cyclistic Bike-Share Analysis"
author: "Daniel Mallia"
output: html_notebook
---

```{r Imports, message=FALSE}
# Note this attaches ggplot2, lubridate, readr, stringr, dplyr and tidyr
library(tidyverse)
library(skimr)
library(naniar)

# Including this line to disable a "friendly" but unhelpful warning we get
# when grouping by multiple columns
# https://stackoverflow.com/questions/62140483/how-to-interpret-dplyr-message-summarise-regrouping-output-by-x-override
options(dplyr.summarise.inform=F)

# Set this to false if you want to download everything for the first time
DOWNLOADED <- TRUE
```


# Purpose
This R notebook contains my end-to-end work on the Cyclistic bike-share case
study, performed as a "capstone" project for Google's excellent Data Analytics
certificate program on Coursera.

The Google program recommends a data analysis process of 6 steps: ask, prepare,
process, analyze, share and act.
Accordingly, this notebook is structured
around those steps.

Please note that some inspiration for the below work comes from an R script,
which was provided in the course and is based on 
[this blog](https://artscience.blog/home/divvy-dataviz-case-study).

# 1. Ask
Let's start by briefly spelling out the (fictional) scenario for this case study
for some context.
I am a junior data analyst (woohoo!) on the marketing analyst team at Cyclistic,
a bike-sharing company in Chicago.
Following the conclusion from the finance analysts at Cyclistic that annual
memberships are more profitable than **casual riders** (who use single ride or
day passes), the director of marketing wants to work on converting casual riders
into **Cyclistic members** who have an annual membership.
This is motivated by the idea that these casual consumers are already aware of
Cyclistic, and may be drawn by the company's offerings of alternative bike types
suitable for those for disabilities.
To support this initiative, the key question for my work to answer with data, is
how do casual riders differ from those with annual memberships?

### Business Task
Make use of the company's historical bike trip data to understand differences
between casual riders and those with annual memberships, and thereby unlock
insights and recommendations for a marketing program designed to convert casual
riders into Cyclistic members.

### Key Stakeholders
- Lily Moreno (Director of Marketing, and my manager)
- Marketing Analytics Team (my colleagues)
- Executive Team (company executives, who have final say on the proposed
marketing program)

# 2. Prepare
### Original Data Organization and Data Retrieval
Because Cyclistic is a fictional company, we are making use of Divvy trip
datasets made publicly available
[here](https://divvy-tripdata.s3.amazonaws.com/index.html) under
[this license](https://www.divvybikes.com/data-license-agreement).
A quick glance at this data repository reveals that the data format has changed
over time, with ride data offered first (in 2013) with a file for a full year,
then in per quarter segments, and then currently by month.
Moreover, the naming scheme has changed in accordance with some of these
changes, and consequently the index is not in chronological order.

For purposes of this analysis, we will work with the most recent full year's
worth of data, the 2022 data.
This requires downloading the 12 files which constitute the 2022 collection.
We will store the original csv files in a subdirectory called "data".

```{r Retrieve Data, message=FALSE, warning=FALSE}
if(!DOWNLOADED) {
  src_dir <- getwd()
  dir.create("data")
  setwd(paste0(src_dir,"/data"))

  for(i in 1:12){
    padded_i <- str_pad(i, 2, pad="0")
    temp_url <- sprintf(
      "https://divvy-tripdata.s3.amazonaws.com/2022%s-divvy-tripdata.zip",
      padded_i)
    destfile <- sprintf("2022%s-divvy-tripdata.zip", padded_i)
    download.file(temp_url, destfile=destfile)
    unzip(destfile)
    file.remove(destfile)
    file.remove("__MACOSX")
  }
}
```

### General Characteristics of the Data
Even before opening a single file, we can make a couple of observations about
the nature of the data we have to work with.
This would constitute **first party data** as (in our fictional scenario) this
is our company's data.
In a real-world scenario, we would ensure that we are retrieving the data
directly from a company source and verifying with data engineers or other
analysts, that this is the correct and up-to-date data.
Nonetheless, we should treat this data as any other, and perform checks both
statistical and visual to ensure data integrity and catch errors.
Simple storage in a sub-directory for our analysis is fine for these purposes;
as we combine data and produce a larger file for the full year's analysis, we
will want to follow good naming conventions and maintain a changelog.

We will be reviewing a full year's worth of data, which should negate the bias
that might arise from seasonal behavior (though we will want to consider
seasonal trends in our analysis).
Moreover, the data is from 2022, which is both recent (always important, but
particularly the case for bike-sharing which is a more recent and growing
trend) and after the major COVID shutdowns, so we should see behavior that is
representative of the present behavior of Cyclistic customers.
It is worth acknowledging that this data is, of course, limited to Cyclistic
customers - and thereby, bike-sharing in a purely Chicago context - so we would
need to be cautious in generalizing outside of the company or to other
localities, but it should be appropriate given our task of considering
differences between *Cyclistic* casual riders and members.

### Inspection of the Data
To dig further, it is time to actually open the files and begin to look for
issues, and compile a file with the full year's worth of data.

One of the files is named "publictripdata" instead of "tripdata", but we can
quickly make that consistent:
```{r}
if(!DOWNLOADED){
  file.rename("data/202209-divvy-publictripdata.csv",
              "data/202209-divvy-tripdata.csv")
}
```


```{r}
tripdata_by_month <- list()
for(i in 1:12){
  padded_i <- str_pad(i, 2, pad="0")
  csv_name <- sprintf("data/2022%s-divvy-tripdata.csv", padded_i)
  tripdata_by_month[[i]] <- read_csv(csv_name)
}
```

The nice thing about reading in csv files with readr is we get a quick
summary, giving rows, columns, the delimiter detected, the titles of columns and
their detected types.
Nonetheless, before we try to combine them into a single dataframe, let's check
all column names are identical.
```{r}
# Check that all column names are identical
trip_colnames <- colnames(tripdata_by_month[[1]])
print(paste("All column names identical:",
  all(sapply(tripdata_by_month, function(x) {identical(colnames(x), trip_colnames)}))))
```

Let's also take a look at the details of at least one month's data.
It would be best to do this for every month but for time and space
considerations, plus the fact that we can perform some checks better in a visual
fashion, we can just run it for January:
```{r}
skim_without_charts(tripdata_by_month[[1]])
```

This output gives us an overall summary of the dataframe, and then statistics
for the character type columns, numeric type columns and finally date type
columns.

To inspect some values, let's use glimpse:
```{r}
glimpse(tripdata_by_month[[1]])
```

We can see we have a ride id, a rideable type (the type of vehicle, such as
"electric_bike" or "classic_bike"), the start and end times of rides (note that
we actually have an end time in February, the "max" time for the January data),
start and end station names and ids, start and end latitudes and longitudes,
and whether or not the rider was a member or a "casual" rider.
Of course, we also have the month column we added in earlier.

We can quickly check the unique values for rideable_type and member_casual:
```{r}
print(unique(tripdata_by_month[[1]]$rideable_type))
print(unique(tripdata_by_month[[1]]$member_casual))
```

**NOTE** Two other items jump out here:

- We have missing values for station names and ids, as well as trip end latitude
and longtidue...

- We have 103770 rows and 103770 unique ride ids, so all seems well for January,
but we should check this holds for all other months.

For our final step, let's combine the dataframes into one:
```{r}
all_tripdata <- bind_rows(tripdata_by_month)
```

We can save this result as a file, so we don't need to repeat these steps in
the future.
```{r}
write_csv(all_tripdata, "2022-divvy-tripdata-V01.csv")
```

# 3. Process
Now we can begin work on all of our 2022 data, getting some sense of the nature
and quality of the data, extracting additional features (columns) and cleaning
data as need be.

First, we can reload the data, if needed.
```{r}
defined <- ls()
if(!("all_tripdata" %in% defined)) {
  all_tripdata <- read_csv("2022-divvy-tripdata-V01.csv")
}
```

Next, let's take a quick look at the top or *head* of the dataset for a
refresh on content.
```{r}
head(all_tripdata)
```

From this we can see we have 13 columns, how about the number of rows for 2022
combined?
Again, we can use skim_without_charts to check this and retrieve a lot more
information all in one call.
```{r}
skim_without_charts(all_tripdata)
```

It looks like all told, we have over 5 million trips to analyze!
That's a nice sample size, one which would likely be overwhelming for
spreadsheet software (Excel / Google Sheets) so it's a good thing we are using
R (though SQL would also be a good option here).

Let's look over the results column by column to get to know the data better.

### Categorical Data
#### ride_id
These are 16 character long ids, and it there are (thankfully) as many unique
ids as there are rows in the data.

#### rideable_type
These give us the actual type of vehicles used; let's see a quick breakdown by
type.
```{r}
table(all_tripdata$rideable_type)
```
Classic and electric bikes clearly make up the vast majority of the data.
The [Divvy website](https://divvybikes.com/system-data) doesn't explain what
a docked bike is, but it sounds like this may not be a normal ride?
We will revisit this below.

#### Station names and ids
Per the names, these are the names (strings) of stations and the ids assigned
to them.
Around 15% of the data is missing here; if we peek ahead, we can see that we
have the majority of latitude and longitude data, so it is certainly not a good
idea to simply discard rows missing these values, particularly when, if desired,
we may be able to recover the correct values using latitude and longitude.
The frequency of missingness seems consistent across name and id, so if we're
missing one, we are missing the other, but we can also check that to be
100% sure.
```{r}
print(paste("Number missing both start name and id:",
  nrow(all_tripdata %>%
    filter(is.na(start_station_name) & is.na(start_station_id)))))
print(paste("Number missing both end name and id:",
  nrow(all_tripdata %>%
    filter(is.na(end_station_name) & is.na(end_station_id)))))
```

Interestingly, there are considerably more names than ids, and a few more end
ids than start ids.
If we have time, we can revisit this to see if we have misspelled (and therefore
duplicated) station names, but this assumes we want to make use of the names.

#### member_casual
This identifies whether or not the ride was by a member or a casual rider
(hence only 2 unique values); let's check the breakdown.
```{r}
table(all_tripdata$member_casual)
prop.table(table(all_tripdata$member_casual))
```
Excellent, while the data does skew in favor of members (~60/40), we also have
quite a sample of casual riders.

### Numerical and Date data
#### Start and end latitude and longitudes
What these offer is no surprise.
As mentioned above, there is a small amount of missingness for end locations;
once we have considered other information we can extract, we'll likely want to
come back and remove any remaining such cases given that we want full rides and
that there are so few cases.

#### Start and end datetimes
Again, per the name, these give us full datetime (year, month, day, hour,
minute, second) for a ride's start and stop.
As we saw with the January data, the max value actually exceeds 2022, as least
one person rode into the new year!
Thankfully, there is no missingness here; this is great news given that we can
make use of these columns to calculate ride durations (not present in these
files).
We can also unpack these further to ask questions about daily or monthly ride
patterns.

### Missingness overview
Before we calculate additional columns, we can get a quick snapshot of what
the missingness looks like in the data, and then revisit it once we've
executed any deletion based on missingness or other problematic values.
To do this, we can use the wonderful naniar package.
Note that there is a [known issue](https://github.com/njtierney/naniar/issues/286)
- also see [here](https://github.com/njtierney/naniar/issues/259) - with naniar
producing an extra blank plot in the previewed html output, so we should only
pay attention to the second plot.


```{r}
gg_miss_upset(all_tripdata, nsets=6)
```
This captures in a nice visual fashion what we saw above and more:
station names and ids are always missing together (in the sense of both start or
end station name and id will be missing, not just name or id).
Some cases have just starts missing, others ends, some both, and there is a
set of almost six thousand cases where the end station information will be
unrecoverable, given that latitude and longitude information is also missing.


### Calculated Data And Cleaning
Now we can add in additional columns based on the datetime columns.
We don't need to account for year, as this is all 2022 data.
In addition to drawing on inspiration from the blog mentioned at the top of
this notebook for generating new columns,
[this blog post](https://www.linkedin.com/pulse/data-analysis-visualizations-chicago-divvy-bikes-sharing-mazarei/)
inspired me to also include hours, so we can consider riding habits during
different parts of the day.
```{r}
all_tripdata <- all_tripdata %>%
  mutate(
    ride_datetime = as_datetime(started_at),
    month = month(ride_datetime, label=T),
    day = day(ride_datetime),
    day_name = wday(ride_datetime, label=T),
    hour = hour(ride_datetime)
  )
head(all_tripdata)
```

And let's add ride durations:
```{r}
# Note for subsequent work, we should convert the results of difftime, instances
# of class "difftime" to numeric; specifically we'll use integers as we don't
# need fractions of seconds, and can then easily check the number of zero
# length trips
all_tripdata <- all_tripdata %>%
  mutate(ride_length = as.integer(difftime(ended_at,started_at), units="secs"))
head(all_tripdata)
```

Now let's perform a couple of checks and start doing some cleaning.
Calling summary on ride_length gives us a place to start.
```{r}
summary(all_tripdata %>% select(ride_length))
```
Clearly there are negative durations present, which should not be preserved.
Communication with other data engineers or analysts may be able to resolve where
these came from and prevent them in the future, but for now we will want to see
how many such cases are present.
Moreover, it stands to reason that there are likely also zero length cases, and,
given that this data has these errors (which are described as having been
removed on [Divvy's website](https://divvybikes.com/system-data)), then we
should also check for another issue they address: trips less than 60 seconds
long.

```{r}
print(paste("Number of rows with negative ride length:",
            nrow(all_tripdata %>% filter(ride_length < 0))))
```

```{r}
print(paste("Number of rows with empty ride length:",
            nrow(all_tripdata %>% filter(ride_length == 0))))
```

```{r}
print(paste("Number of rows with positive ride length less than 60 seconds:",
            nrow(all_tripdata %>% filter(ride_length > 0 & ride_length < 60))))
```

The blog which is of some inspiration to this analysis mentions a separate but
related issue, where some rides start at station "HQ QR", when the ride was
in fact an inspection.
However, the below shows that there are no such cases in the 2022 data
```{r}
nrow(all_tripdata %>% filter(start_station_name == "HQ QR"))
```

Let's delete rows with bad trip durations, creating a new dataframe with the
result:
```{r}
start_count <- nrow(all_tripdata)
all_tripdata_v2 <- all_tripdata %>%
  filter(ride_length >= 60)
print(paste("Removed", start_count - nrow(all_tripdata_v2), "rows"))
```

```{r}
print(paste("Number of rows now missing end lat/lon:",
  nrow(all_tripdata_v2 %>%
    filter(is.na(end_lat) & is.na(end_lng)))))
```

Interestingly, this shows us that focusing on duration alone did not address
the cases with missing end latitude and longitude.
Let's remove those now:
```{r}
start_count <- nrow(all_tripdata_v2)
all_tripdata_v2 <- all_tripdata_v2 %>%
  filter(!(is.na(end_lat) & is.na(end_lng)))
print(paste("Removed", start_count - nrow(all_tripdata_v2), "rows"))
```

Checking the distribution of rideable_type after these removals shows that
"docked_bike" does not in fact correspond to an erroneous ride type.
In fact, this
[blog](https://www.linkedin.com/pulse/data-analysis-visualizations-chicago-divvy-bikes-sharing-mazarei/)
suggests "docked_bike" might be an older name for "classic_bike".
Moreover, the number of these instances is comparatively small, so we will
leave them as is.
```{r}
table(all_tripdata_v2$rideable_type)
```


Finally, let's revisit the missingness overview.
```{r}
gg_miss_upset(all_tripdata_v2, nsets=6)
```

Here we write out the intermediate result after our processing and cleaning.
```{r}
write_csv(all_tripdata_v2, "2022-divvy-tripdata-V02.csv")
```

In addition, for resuming our work in this notebook it is best to save out the
data as an RDS object.
Otherwise, if we try to reload the data only from csv, we will need to do
extra work to convert day_name and month back into ordered form, as they will
be interpreted as just characters.
```{r}
saveRDS(all_tripdata_v2, "2022-divvy-tripdata-V02.RDS")
```



# 4. Analyze
Again, we can reload the data, if needed.
```{r}
defined <- ls()
if(!("all_tripdata_v2" %in% defined)) {
  all_tripdata_v2 <- readRDS("2022-divvy-tripdata-V02.RDS")
}
```

## Member types
Now that we have our cleaned data, let's first look at the numbers and
proportions of rides by rider types:
```{r}
table(all_tripdata_v2$member_casual)
prop.table(table(all_tripdata_v2$member_casual))
```
Thankfully the deletions executed in the cleaning phase have not fundamentally
altered the proportions of rides.

## Ride Durations
As a first step, we can consider the minimum, maximum, mean and median ride
lengths.
Let's look at this overall...
```{r}
summary(all_tripdata_v2$ride_length)
```

...and now broken down by casual riders vs. members.
```{r}
all_tripdata_v2 %>%
  group_by(member_casual) %>%
  summarize(min=min(ride_length), max=max(ride_length), mean=mean(ride_length),
            median=median(ride_length))
```
The fact that the mean is significantly greater than the median overall
and for both groups suggests there is considerable rightward skew, with the
mean being inflated by outlier long rides. Considering seconds alone
is difficult, so let's view this by minutes.

```{r}
all_tripdata_v2 <- all_tripdata_v2 %>%
  mutate(ride_length_mins=(ride_length / 60.0))
all_tripdata_v2 %>%
  group_by(member_casual) %>%
  summarize(min=min(ride_length_mins), max=max(ride_length_mins),
            mean=mean(ride_length_mins), median=median(ride_length_mins))
```
There are 1440 minutes in a day, so the above shows us that the max any member
"rode" was for a little over a day; for casual riders the max is over 20 days...
Let's check how many rides overall were for casual members, and then how many of
those exceeded a day.

```{r}
total_casual <- nrow(all_tripdata_v2 %>% filter(member_casual == "casual"))
total_casual_exceeding_day <- nrow(
  all_tripdata_v2 %>%
    filter(member_casual == "casual" & ride_length_mins > 1440))
total_casual
total_casual_exceeding_day
```

Out of over 2 million casual rides, only `r total_casual_exceeding_day`
exceeded a day; these few cases are unlikely to tell us much about casual
riders, so let's exclude the cases that exceed the member max, and reconsider
the statistics.

```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500) %>%
  group_by(member_casual) %>%
  summarize(min=min(ride_length_mins), max=max(ride_length_mins),
            mean=mean(ride_length_mins), median=median(ride_length_mins))
```

Inspecting this visually is tricky given not only the outliers, but the very
broad range of values. One option is to constrain the range further to get a
better sense of distributions.
```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 120) %>%
  ggplot(aes(x=ride_length_mins)) + geom_density(aes(fill=member_casual), alpha=0.5)
```
This plot does confirm that the distribution for members has a sharper peak
covering the range of several minute rides, whereas casual riders clearly skew
towards longer rides.
We can hypothesize that this may reflect ride motivations: members may be
primarily using rides for short trips to and from work, or social gatherings,
whereas more casual riders may be using bikes for longer outings (an end in
themselves).

## Day Behavior
A quick starting place for understanding behavior by day is to examine the
frequency of rides by day, broken into casual and member rides. NOTE: this plot
and others like it below require careful interpretation, given that the
distribution of member rides to casual rides is not 50/50, but in fact closer to
60/40. That said, we can still get a feeling for the different distributions
side by side.
```{r}
ggplot(all_tripdata_v2, aes(x=day_name)) +
  geom_bar(aes(fill=member_casual), position="dodge")
```
The above gives us some indication that more casual rides occur on weekends
than weekdays; members rides are somewhat more evenly distributed but do
emphasize weekday use.
Again, this does suggest difference of reasons for usage.

Let's look at the fine-toothed breakdown of statistics by day.
It turns out that the minimum allowed (1 minute) occurs every day of the week,
and values close to the max (again we limit our analysis to the max member ride
length), so let's just retrieve mean and median.
We group by day_name first so as to compare values day by day.
```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500) %>%
  group_by(day_name, member_casual) %>%
  summarize(mean(ride_length_mins), median(ride_length_mins))
```
We can also examine this visually (let's just examine median):
```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500) %>%
  group_by(day_name, member_casual) %>%
  summarize(median=median(ride_length_mins)) %>%
  ggplot(aes(x=day_name, y=median)) +
  geom_col(aes(fill=member_casual), position="dodge")
```

Overall, the actual per-ride statistics conform to the picture given by
ride counts per day (as well as the raw ride statistics we examined before),
where overall casual users go for longer rides and members and casual riders
differ on how they behave by day.
Member ride statistics don't vary as much day by day (though durations do
increase a little on weekends, perhaps when rides can be more leisurely),
whereas casual rides do shorten by multiple minutes (e.g. consider Sunday casual
median of ~15 minutes vs. Tuesday casual median of ~12 minutes).
Geographic and distance traveled information may help provide more information
on what is happening in the casual group, **but** this does suggest
that there may be multiple groups within casual riders:

- a (perhaps small) group that are more likely to ride on any day of the week
and who are responsible for the shorter, consistent behaviors on weekdays;

- and a likely larger group who primarily ride for leisure on weekends.
The first group would be the ideal targets of a campaign for converting
casual riders to members!

## Monthly/Seasonal Behavior
Again, for monthly behavior as for daily behavior, we can by visualizing the
counts of rides by month (heeding the aforementioned warning about the
proportions of casual rides to member rides).
```{r}
ggplot(all_tripdata_v2, aes(x=month)) +
  geom_bar(aes(fill=member_casual), position="dodge")
```
Unsurprisingly, the months of December, January and February are noticeably
less popular for members and casual riders, as these are winter months in
North America, and many will seek other options for transportation in the face
of low temperatures, snow and ice.
Warmer and safer weather months certainly see a lot more bike usage, seeing
increases by a factor of 4+.
Yet, members again seem to evidence more consistency (as with their ride
durations and use across the week), having a less peaked distribution and in
fact the months with maximum use for are later than for casual riders.

Let's consider statistics by month:
```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500) %>%
  group_by(month, member_casual) %>%
  summarize(mean=mean(ride_length_mins), median=median(ride_length_mins))
```
Again, we can also visualize this (for the median):
```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500) %>%
  group_by(month, member_casual) %>%
  summarize(median=median(ride_length_mins)) %>%
  ggplot(aes(x=month, y=median)) +
  geom_col(aes(fill=member_casual), position="dodge")
```
As with ridership by day, it seems there are two groups of riders among casual
rides, and this is worth mining further.

## Hourly Behavior
Days were an intuitive starting place, and months give us a higher level
overview, but we can also zoom in further to consider hours of usage.
It is an interesting avenue to mine, but is somewhat difficult to review and
plot, given that we want to consider counts by start and end hour pairs, and
our analysis should be flexible enough to consider where rides may exceed the
boundary of an hour.

One way to examine hours of usage is to make a plot counting the frequency for
each combination of a ride start hour and end hour.
We can easily create this 24x24 grid using ggplot's geom_count and this
has the advantage over a heatmap approach in that we can make use of both size
and color here: in the below we will use size to indicates raw count, and color
to capture proportion.
We make separate plots for casual rides and member rides, as otherwise the
plots will become too small to interpret.
```{r}
# Some setup for nice axis labels
breaks <- seq(0,23,3) # Breaks to display on the axis
hrs <- c(12,3,6,9)
# Create 12hr clock labels
labels <- c(paste0(hrs, "AM"), paste0(hrs, "PM"))
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500 & member_casual == "casual") %>%
  mutate(end_hour = hour(ended_at)) %>%
  ggplot(aes(x=hour, y=end_hour)) +
  geom_count(aes(color=after_stat(prop))) +
  scale_x_continuous(breaks = breaks, labels=labels) +
  scale_y_continuous(breaks = breaks, labels=labels) +
  labs(title="Counts for Casual Rides")
```
The above does require some careful reading and interpretation.
Moreover, it is complicated by the fact that you can have rides that last for
multiple hours before midnight (all values above the y=x line) or even rides
which carry over into the subsequent day (all values below the y=x line).
Nevertheless, the plot gives a fascinating window into hourly trends:

1. First, the fact that the majority of rides are concentrated around the y=x
line, or a little above the line, confirms that the majority of rides are short.
The ride begins in the hour and ends in the same hour or perhaps the next.
2. Unsurprisingly, many rides occur in the daylight hours, but there are also
clearly plenty of night owls among casual riders, with bikes being taken out and
returned at all times of the day.
3. There is something of a peak in usage around 5PM, which is somewhat
unexpected given that these are casual riders. Again, this suggests that there
is likely a group among them that do use these bikes somewhat regularly,
perhaps for commuting home, and that would constitute possible converts to
Cyclistic members.

```{r}
all_tripdata_v2 %>%
  filter(ride_length_mins <= 1500 & member_casual == "member") %>%
  mutate(end_hour = hour(ended_at)) %>%
  ggplot(aes(x=hour, y=end_hour)) +
  geom_count(aes(color=after_stat(prop))) +
  scale_x_continuous(breaks = breaks, labels=labels) +
  scale_y_continuous(breaks = breaks, labels=labels) +
  labs(title="Counts for Member Rides")
```
The story for members has some clear differences (though as with many such
plots, we need to heed the difference in ride counts between members and
casual riders).

1. The areas far above and below the y=x line are much more sparse, while the
y=x line (and a little above) itself remains a strong concentration.
This suggests more consistent, regular usage with less instances of bikes being
out for many hours or even into the next day.
2. We see an even stronger concentration around 5PM for members than was
present for casual rides, as well as a considerable number in the morning,
around 8AM.
This would be consistent with use for rides to or from work.

## Geographic Behavior

```{r}

```


## Distance Behavior


By member/casual:
- Ride trip length stats (mean/median/min/max)...DONE
- Trips by day - count, and trip stats (mean/median/min/max)...DONE
- Trips by month - count, and trip stats (mean/median/min/max)...DONE
- Trip distance - use manhattan distance (would need to add additional column)
- Consider times of day? Added in hour to support this

Plot member/casual proportions by station

ADD IN START AND END HOUR, REMOVE DAY
DELETE ROWS WITH INCORRECT lat/long

# 5. Share

Can use polished versions of some of the above visualizations.

# 6. Act

- Need more information about the rider types and information from the
riders themselves:
  - How many of the casual riders are single ride vs daily pass?
  - What is their primary reason for using the bike?
- Need a financial breakdown

